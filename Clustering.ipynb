{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5189e59b-2ddc-4c3f-ae55-cebc0e8f21c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original K-Means Results:\n",
      "  SSE: 41038704.06121276\n",
      "  Silhouette Score: 0.18035134956090837\n",
      "  Dunn Index: 0.015027991559122982\n",
      "  BetaCV: 0.5203663740886495\n",
      "  Hubert’s Statistic: -0.37318867525848504\n",
      "\n",
      "\n",
      "Transformed K-Means Results:\n",
      "  SSE: 53861.94751656399\n",
      "  Silhouette Score: 0.16326744911533145\n",
      "  Dunn Index: 0.026381297333790715\n",
      "  BetaCV: 0.35183007514812903\n",
      "  Hubert’s Statistic: -0.34452132007840824\n",
      "\n",
      "\n",
      "Original EM Results:\n",
      "  SSE: 49455517.48023911\n",
      "  Silhouette Score: 0.02612281443633195\n",
      "  Dunn Index: 0.011765458201096927\n",
      "  BetaCV: 0.17711686580366184\n",
      "  Hubert’s Statistic: -0.1346768898545876\n",
      "\n",
      "\n",
      "Transformed EM Results:\n",
      "  SSE: 60161.67663319381\n",
      "  Silhouette Score: 0.034827172290873794\n",
      "  Dunn Index: 0.018652505359670838\n",
      "  BetaCV: 0.09133128520935428\n",
      "  Hubert’s Statistic: -0.12088872476640357\n",
      "\n",
      "\n",
      "Original K-Medoids Results:\n",
      "  SSE: 38965282.268565\n",
      "  Silhouette Score: 0.2429475277793729\n",
      "  Dunn Index: 0.026366396730414538\n",
      "  BetaCV: 0.806993251578068\n",
      "  Hubert’s Statistic: -0.48055768329421783\n",
      "\n",
      "\n",
      "Transformed K-Medoids Results:\n",
      "  SSE: 60651.186246430254\n",
      "  Silhouette Score: 0.0712254981314364\n",
      "  Dunn Index: 0.02613902640034805\n",
      "  BetaCV: 0.2428423492779275\n",
      "  Hubert’s Statistic: -0.26895802654871603\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Clustering:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "\n",
    "# Custom K-Means Implementation\n",
    "def kmeans_custom(data, n_clusters, max_iters=100, tol=1e-4):\n",
    "    n_samples, n_features = data.shape\n",
    "    centroids = data[np.random.choice(n_samples, n_clusters, replace=False)]\n",
    "    prev_centroids = centroids.copy()\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # Assign labels based on closest centroid\n",
    "        distances = cdist(data, centroids, 'euclidean')\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Compute new centroids\n",
    "        new_centroids = np.array([data[labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "\n",
    "        # Check convergence\n",
    "        if np.all(np.abs(new_centroids - prev_centroids) < tol):\n",
    "            break\n",
    "        prev_centroids = new_centroids\n",
    "\n",
    "    return labels, new_centroids\n",
    "\n",
    "# Custom Expectation-Maximization (EM) Clustering Implementation (Gaussian Mixture Model)\n",
    "def em_custom(data, n_components, max_iters=100, tol=1e-4):\n",
    "    n_samples, n_features = data.shape\n",
    "    # Initialize the parameters (random initialization)\n",
    "    means = data[np.random.choice(n_samples, n_components, replace=False)]\n",
    "    covariances = np.array([np.cov(data.T)] * n_components)\n",
    "    weights = np.ones(n_components) / n_components\n",
    "    prev_log_likelihood = None\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # E-step: calculate responsibilities (posterior probabilities)\n",
    "        responsibilities = np.array([weights[k] * multivariate_gaussian(data, means[k], covariances[k]) for k in range(n_components)])\n",
    "        responsibilities /= responsibilities.sum(axis=0)\n",
    "\n",
    "        # M-step: update parameters based on responsibilities\n",
    "        Nk = responsibilities.sum(axis=1)\n",
    "        means = (responsibilities @ data) / Nk[:, np.newaxis]\n",
    "        covariances = np.array([np.cov(data.T, aweights=responsibilities[k]) for k in range(n_components)])\n",
    "        weights = Nk / n_samples\n",
    "\n",
    "        # Calculate log likelihood\n",
    "        log_likelihood = np.sum(np.log(np.sum(responsibilities, axis=0)))\n",
    "        if prev_log_likelihood is not None and np.abs(log_likelihood - prev_log_likelihood) < tol:\n",
    "            break\n",
    "        prev_log_likelihood = log_likelihood\n",
    "\n",
    "    labels = np.argmax(responsibilities, axis=0)\n",
    "    return labels, means, covariances, weights\n",
    "\n",
    "# Helper function for multivariate Gaussian distribution\n",
    "def multivariate_gaussian(data, mean, cov):\n",
    "    n_features = data.shape[1]\n",
    "    diff = data - mean\n",
    "    return np.exp(-0.5 * np.sum(np.dot(diff, np.linalg.inv(cov)) * diff, axis=1)) / np.sqrt((2 * np.pi) ** n_features * np.linalg.det(cov))\n",
    "\n",
    "# Custom K-Medoids Implementation\n",
    "def kmedoids_custom(data, n_clusters, max_iters=100):\n",
    "    n_samples = data.shape[0]\n",
    "    # Randomly initialize medoids (choosing n_clusters data points)\n",
    "    medoids = data[np.random.choice(n_samples, n_clusters, replace=False)]\n",
    "    prev_medoids = medoids.copy()\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # Assign labels based on closest medoid\n",
    "        distances = cdist(data, medoids, 'euclidean')\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Recalculate medoids\n",
    "        new_medoids = np.array([data[labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "        new_medoids = new_medoids.round().astype(int)  # Round to nearest point for K-medoids\n",
    "\n",
    "        # Check convergence\n",
    "        if np.all(np.abs(new_medoids - prev_medoids) == 0):\n",
    "            break\n",
    "        prev_medoids = new_medoids\n",
    "\n",
    "    return labels, new_medoids\n",
    "\n",
    "# Compute metrics\n",
    "def compute_sse(data, labels, centroids):\n",
    "    \"\"\"Compute Sum of Squared Errors (SSE).\"\"\"\n",
    "    # Compute pairwise distances from data points to centroids\n",
    "    distances = cdist(data, centroids, 'euclidean')\n",
    "    \n",
    "    # Find the minimum distance (for each point) and square it\n",
    "    sse = np.sum(np.min(distances ** 2, axis=1))\n",
    "    \n",
    "    return sse\n",
    "\n",
    "def compute_silhouette(data, labels):\n",
    "    \"\"\"Compute Silhouette Score.\"\"\"\n",
    "    return silhouette_score(data, labels) if len(np.unique(labels)) > 1 else np.nan\n",
    "\n",
    "def compute_beta_cv(data, labels):\n",
    "    \"\"\"Compute BetaCV.\"\"\"\n",
    "    clusters = np.unique(labels)\n",
    "    cluster_means = np.array([data[labels == cluster].mean(axis=0) for cluster in clusters])\n",
    "    overall_mean = data.mean(axis=0)\n",
    "\n",
    "    inter_cluster_var = np.sum([len(data[labels == cluster]) * np.sum((mean - overall_mean) ** 2) for cluster, mean in enumerate(cluster_means)])\n",
    "    intra_cluster_var = np.sum([np.sum((data[labels == cluster] - mean) ** 2) for cluster, mean in enumerate(cluster_means)])\n",
    "\n",
    "    return inter_cluster_var / intra_cluster_var\n",
    "\n",
    "def compute_dunn_index(data, labels):\n",
    "    \"\"\"Compute Dunn Index.\"\"\"\n",
    "    clusters = np.unique(labels)\n",
    "    distances = cdist(data, data)\n",
    "    min_intercluster_dist = np.inf\n",
    "    max_intracluster_dist = 0\n",
    "\n",
    "    for i in clusters:\n",
    "        cluster_points = np.where(labels == i)[0]\n",
    "        intracluster_dist = np.max(distances[np.ix_(cluster_points, cluster_points)])\n",
    "        max_intracluster_dist = max(max_intracluster_dist, intracluster_dist)\n",
    "\n",
    "        for j in clusters:\n",
    "            if i != j:\n",
    "                other_cluster_points = np.where(labels == j)[0]\n",
    "                intercluster_dist = np.min(distances[np.ix_(cluster_points, other_cluster_points)])\n",
    "                min_intercluster_dist = min(min_intercluster_dist, intercluster_dist)\n",
    "\n",
    "    return min_intercluster_dist / max_intracluster_dist\n",
    "\n",
    "def compute_huberts_statistic(data, labels):\n",
    "    \"\"\"Compute Hubert’s Statistic.\"\"\"\n",
    "    distance_matrix = pdist(data, metric='euclidean')\n",
    "    cluster_matrix = np.equal.outer(labels, labels).astype(int)\n",
    "    flat_distances = squareform(distance_matrix)\n",
    "    return np.corrcoef(flat_distances.flatten(), cluster_matrix.flatten())[0, 1]\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove categorical attributes and target variable\n",
    "numerical_data = data.select_dtypes(include=['float64', 'int64'])\n",
    "target_variable = 'temp'\n",
    "\n",
    "# Fill missing values with mode\n",
    "for column in numerical_data.columns:\n",
    "    mode_value = numerical_data[column].mode()[0]\n",
    "    numerical_data[column] = numerical_data[column].fillna(mode_value)\n",
    "\n",
    "X_original = numerical_data.drop(columns=[target_variable]).values\n",
    "\n",
    "# Apply Z-score transformation\n",
    "scaler = StandardScaler()\n",
    "X_transformed = scaler.fit_transform(X_original)\n",
    "\n",
    "# Number of clusters\n",
    "k = 3\n",
    "\n",
    "# K-Means clustering\n",
    "kmeans_original_labels, kmeans_original_centroids = kmeans_custom(X_original, k)\n",
    "kmeans_transformed_labels, kmeans_transformed_centroids = kmeans_custom(X_transformed, k)\n",
    "\n",
    "# EM Clustering\n",
    "em_original_labels, em_original_means, em_original_covariances, em_original_weights = em_custom(X_original, k)\n",
    "em_transformed_labels, em_transformed_means, em_transformed_covariances, em_transformed_weights = em_custom(X_transformed, k)\n",
    "\n",
    "# K-Medoids clustering\n",
    "kmedoids_original_labels, kmedoids_original_centroids = kmedoids_custom(X_original, k)\n",
    "kmedoids_transformed_labels, kmedoids_transformed_centroids = kmedoids_custom(X_transformed, k)\n",
    "\n",
    "# Prepare results\n",
    "clustering_results = [\n",
    "    (\"Original K-Means\", X_original, kmeans_original_labels, kmeans_original_centroids),\n",
    "    (\"Transformed K-Means\", X_transformed, kmeans_transformed_labels, kmeans_transformed_centroids),\n",
    "    (\"Original EM\", X_original, em_original_labels, em_original_means),\n",
    "    (\"Transformed EM\", X_transformed, em_transformed_labels, em_transformed_means),\n",
    "    (\"Original K-Medoids\", X_original, kmedoids_original_labels, kmedoids_original_centroids),\n",
    "    (\"Transformed K-Medoids\", X_transformed, kmedoids_transformed_labels, kmedoids_transformed_centroids),\n",
    "]\n",
    "\n",
    "# Compute evaluation metrics\n",
    "for name, data, labels, centroids in clustering_results:\n",
    "    sse = compute_sse(data, labels, centroids) if centroids is not None else \"N/A\"\n",
    "    silhouette = compute_silhouette(data, labels)\n",
    "    beta_cv = compute_beta_cv(data, labels)\n",
    "    dunn_index = compute_dunn_index(data, labels)\n",
    "    huberts_statistic = compute_huberts_statistic(data, labels)\n",
    "\n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  SSE: {sse}\")\n",
    "    print(f\"  Silhouette Score: {silhouette}\")\n",
    "    print(f\"  Dunn Index: {dunn_index}\")\n",
    "    print(f\"  BetaCV: {beta_cv}\")\n",
    "    print(f\"  Hubert’s Statistic: {huberts_statistic}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47dd4b0-9b4f-4c62-80b4-b4b31503a450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
